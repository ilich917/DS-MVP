{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 20\n",
    "dim_model = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=window):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import urllib\n",
    "import gc\n",
    "import datetime\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 15)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    def read_video(self, video_file):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Data(Dataset):\n",
    "    \"\"\"Dataset Class for Loading Video\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, path_videos: str, path_kp: str, train: bool = True, transform = None, time_depth, x_size, y_size, mean):\n",
    "        self.transform = transform\n",
    "        self.df = df\n",
    "        self.path = path_videos # main fold of videos\n",
    "        self.path_kp = path_kp\n",
    "        self.train = train\n",
    "        self.x_size = x_size\n",
    "        self.y_size = y_size\n",
    "        self.time_depth = time_depth\n",
    "        self.mean = mean\n",
    "\n",
    "     def read_video(self, video_file):\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        frames = torch.FloatTensor(self.time_depth, self.x_size, self.y_size)\n",
    "        failed_clip = False\n",
    "        for f in range(self.time_depth):\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame_torch = torch.from_numpy(gray)\n",
    "                frames[f, :, :] = frame_torch\n",
    "\n",
    "            else:\n",
    "                print(\"Skipped!\")\n",
    "                failed_clip = True\n",
    "                break\n",
    "\n",
    "        return frames, failed_clip\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Path of videos: \n",
    "        # Example for 1 video: Enero_body_1.mp4, Enero_face_1.mp4, Enero_handl_1.mp4, Enero_handr_1.mp4\n",
    "\n",
    "        body_path = self.path + '/' + self.df.iloc[index]['target'] + '/' + self.df.iloc[index]['target'] + '_body_' + self.df.iloc[index]['example'] + '.mp4'\n",
    "        face_path = self.path + '/' + self.df.iloc[index]['target'] + '/' + self.df.iloc[index]['target'] + '_face_' + self.df.iloc[index]['example'] + '.mp4'\n",
    "        handl_path = self.path + '/' + self.df.iloc[index]['target'] + '/' + self.df.iloc[index]['target'] + '_handl_' + self.df.iloc[index]['example'] + '.mp4'\n",
    "        handr_path = self.path + '/' + self.df.iloc[index]['target'] + '/' + self.df.iloc[index]['target'] + '_handr_' + self.df.iloc[index]['example'] + '.mp4'\n",
    "\n",
    "        # Read the video\n",
    "        body, failed_body = self.read_video(body_path)\n",
    "        face, failed_face = self.read_video(face_path)\n",
    "        handl, failed_handl = self.read_video(face_path)\n",
    "        handr, failed_handr = self.read_video(vid_path)\n",
    "\n",
    "        # Read the kp matrix for each frame of each video\n",
    "        body_kp = np.array()\n",
    "        face_kp = np.array(self.df.iloc[index][self.context].values, dtype=np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            body = self.transform(body)\n",
    "            face = self.transform(face)\n",
    "            handl = self.transform(handl)\n",
    "            handr = self.transform(handr)\n",
    "        \n",
    "        data = (body, face, handl, handr)\n",
    "        keypoints = (body_kp, face_kp, handl_kp, handr_kp)\n",
    "       \n",
    "        if self.train:\n",
    "            y = self.df.iloc[index]['target']\n",
    "            return (x, meta), y\n",
    "        else:\n",
    "            return (x, meta)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = models.inception_v3(init_weights=False)\n",
    "pretrained_weights = torch.load('../input/torchvision-inception-v3-imagenet-pretrained/inception_v3_google-1a9a5a14.pth', map_location='cpu')\n",
    "pretrained.load_state_dict(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "#device = xm.xla_device() #for tpu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # for gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "paciencia = 3\n",
    "\n",
    "oof = np.zeros((len(train), 1))  # Out Of Fold predictions\n",
    "preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)  # Predictions for test test\n",
    "\n",
    "#kf = GroupKFold(n_splits=4)\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body, face, handl, handr, label = data #inception input dimensions of 299x299 \n",
    "body_kp, face_kp, handl_kp, handr_kp = keypoints #csv #each cell is a vector (4 features) of keypoints all/face/handl/handr normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Data(df=test,\n",
    "                       path = '../input/jpeg-melanoma-256x256/test', \n",
    "                       train=False,\n",
    "                       preprocesar=preprocesar,\n",
    "                       context=context)"
   ]
  },
  {
   "source": [
    "## I've not included yet the pose/kp generation in this network, but I can."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, pretrained, N_vocabulary: int):\n",
    "        super(Neural_Network, self).__init__()\n",
    "\n",
    "        self.body = pretrained\n",
    "        self.face = pretrained\n",
    "        self.handl = pretrained\n",
    "        self.handr = pretrained\n",
    "\n",
    "\n",
    "        #if 'ResNet' in str(pretrained.__class__):\n",
    "        #self.pretrained.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
    "\n",
    "        if 'Inception3' in str(pretrained.__class__):\n",
    "            self.body.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.body.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "\n",
    "            self.face.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.face.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "       \n",
    "            self.handl.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.handl.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "       \n",
    "            self.handr.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.handr.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "        \n",
    "        self.body_plus_kp = nn.Sequential(nn.Linear(512 + n_body_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "            \n",
    "        self.face_plus_kp = nn.Sequential(nn.Linear(512 + n_face_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "        \n",
    "        self.handl_plus_kp = nn.Sequential(nn.Linear(512 + n_handl_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "        \n",
    "        self.handr_plus_kp = nn.Sequential(nn.Linear(512 + n_handr_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.Linear(1024, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 128),\n",
    "                                  nn.BatchNorm1d(128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            TransformerModel(window, 128, 8, 128, 4, 0.1),\n",
    "                                  nn.Linear(128, N_vocabulary))\n",
    "\n",
    "                                  \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        The input is a 20 frames video plus kp for each frame. \n",
    "        This is good, because when we will test this in longer or real time video,\n",
    "        then we just need to feed it with chunks of 20 frames to make predictions.\n",
    "        When frames in video are <20, then padding (when video is starting, for example) # NOT IMPLEMENTED YET\n",
    "        \"\"\"\n",
    "              \n",
    "        n = window # num of frames by video / to be defined\n",
    "\n",
    "        #tensor to save the output of encoder for each frame\n",
    "        distilled_video = torch.zeros(n, 128) # (n_frames, output_encoder)\n",
    "\n",
    "        #videos of same length:\n",
    "        body, face, handl, handr = inputs[0]\n",
    "\n",
    "        # Following is the list of kp for each frame for each video [[kp_frame_1],[kp_frame_2],...,[kp_frame_n]]\n",
    "        # where kp_frame_i is the list of kp coordinates normalized\n",
    "        body_kp, face_kp, handl_kp, handr_kp = inputs[1]\n",
    "        i = 0\n",
    "        while i < n-1:\n",
    "            #CNN forward\n",
    "            body_cnn = self.body(body[i])\n",
    "            face_cnn = self.face(face[i])\n",
    "            handl_cnn = self.handl(handl[i])\n",
    "            handr_cnn = self.handr(handr[i])\n",
    "\n",
    "            #forward for CNN_output + kp\n",
    "            body_pose = self.body_plus_kp(torch.cat((body_cnn, body_kp), dim=1))\n",
    "            face_pose = self.face_plus_kp(torch.cat((face_cnn, face_kp), dim=1))\n",
    "            handl_pose = self.handl_plus_kp(torch.cat((handl_cnn, handl_kp), dim=1))\n",
    "            handr_pose = self.handr_plus_kp(torch.cat((handr_cnn, handr_kp), dim=1))\n",
    "\n",
    "            #forward for encoding the concatenation of latest forwards\n",
    "            distilled_video[i] = self.encoder(torch.cat(body_pose, face_pose, handl_pose, handr_pose))\n",
    "            i += 1\n",
    "        \n",
    "        output = F.softmax(self.output(distilled_video))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (id_train, id_val) in enumerate(kf.split(X=np.zeros(len(train)), y=train['target']), 1):\n",
    "    print('=' * 20, 'Fold', fold, '=' * 20) \n",
    "    model_path = f'model_{fold}.pth'  # Path and filename to save model to\n",
    "    best_val = 0  # Best validation score within this fold\n",
    "    model = Red_Neuronal(pretrained=pretrained, n_meta_features=len(context))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer=optim, mode='max', patience=1, verbose=True, factor=0.2)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_data = Data(df=train.iloc[id_train].reset_index(drop=True), \n",
    "                            path='../input/jpeg-melanoma-256x256/train/', \n",
    "                            train=True, \n",
    "                            preprocesar=preprocesar,\n",
    "                            context=context)\n",
    "    val = Data(df=train.iloc[id_val].reset_index(drop=True), \n",
    "                            path='../input/jpeg-melanoma-256x256/train/', \n",
    "                            train=True, \n",
    "                            preprocesar=preprocesar,\n",
    "                            context=context)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(dataset=val, batch_size=16, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=16, shuffle=False, num_workers=2)\n",
    "    \n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        correct = 0\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x[0] = torch.tensor(x[0], device=device, dtype=torch.float32)\n",
    "            x[1] = torch.tensor(x[1], device=device, dtype=torch.float32)\n",
    "            y = torch.tensor(y, device=device, dtype=torch.float32)\n",
    "            optim.zero_grad()\n",
    "            z, aux = model(x)\n",
    "            \n",
    "            outputs, aux_outputs = model(inputs)\n",
    "            loss1 = criterion(z, y.unsqueeze(1))\n",
    "            loss2 = criterion(aux, y.unsqueeze(1))\n",
    "            loss = loss1 + 0.4*loss2\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            pred = torch.round(torch.sigmoid(z))  # round off sigmoid to obtain predictions\n",
    "            correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()  # tracking number of correctly predicted samples\n",
    "            epoch_loss += loss.item()\n",
    "        train_acc = correct / len(id_train)\n",
    "        \n",
    "        model.eval()  # switch model to the evaluation mode\n",
    "        val_preds = torch.zeros((len(id_val), 1), dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n",
    "            # Predicting on validation set\n",
    "            for j, (x_val, y_val) in enumerate(val_loader):\n",
    "                x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
    "                x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
    "                y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
    "                z_val = model(x_val)\n",
    "                val_pred = torch.sigmoid(z_val)\n",
    "                val_preds[j*val_loader.batch_size:j*val_loader.batch_size + x_val[0].shape[0]] = val_pred\n",
    "            val_acc = accuracy_score(train.iloc[id_val]['target'].values, torch.round(val_preds.cpu()))\n",
    "            val_roc = roc_auc_score(train.iloc[id_val]['target'].values, val_preds.cpu())\n",
    "            \n",
    "            print('Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}'.format(\n",
    "            epoch + 1, \n",
    "            epoch_loss, \n",
    "            train_acc, \n",
    "            val_acc, \n",
    "            val_roc, \n",
    "            str(datetime.timedelta(seconds=time.time() - start_time))[:7]))\n",
    "            \n",
    "            scheduler.step(val_roc)\n",
    "                \n",
    "            if val_roc >= best_val:\n",
    "                best_val = val_roc\n",
    "                patience = es_patience  # Resetting patience since we have new best validation accuracy\n",
    "                torch.save(model, model_path)  # Saving current best model\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    print('Early stopping. Best Val roc_auc: {:.3f}'.format(best_val))\n",
    "                    break\n",
    "                \n",
    "    model = torch.load(model_path)  # Loading best model of this fold\n",
    "    model.eval()  # switch model to the evaluation mode\n",
    "    val_preds = torch.zeros((len(id_val), 1), dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        # Predicting on validation set once again to obtain data for OOF\n",
    "        for j, (x_val, y_val) in enumerate(val_loader):\n",
    "            x_val[0] = torch.tensor(x_val[0], device=device, dtype=torch.float32)\n",
    "            x_val[1] = torch.tensor(x_val[1], device=device, dtype=torch.float32)\n",
    "            y_val = torch.tensor(y_val, device=device, dtype=torch.float32)\n",
    "            z_val = model(x_val)\n",
    "            val_pred = torch.sigmoid(z_val)\n",
    "            val_preds[j*val_loader.batch_size:j*val_loader.batch_size + x_val[0].shape[0]] = val_pred\n",
    "        oof[id_val] = val_preds.cpu().numpy()\n",
    "        \n",
    "        # Predicting on test set\n",
    "        tta_preds = torch.zeros((len(test), 1), dtype=torch.float32, device=device)\n",
    "        for _ in range(TTA):\n",
    "            for i, x_test in enumerate(test_loader):\n",
    "                x_test[0] = torch.tensor(x_test[0], device=device, dtype=torch.float32)\n",
    "                x_test[1] = torch.tensor(x_test[1], device=device, dtype=torch.float32)\n",
    "                z_test = model(x_test)\n",
    "                z_test = torch.sigmoid(z_test)\n",
    "                tta_preds[i*test_loader.batch_size:i*test_loader.batch_size + x_test[0].shape[0]] += z_test\n",
    "        preds += tta_preds / TTA\n",
    "    \n",
    "preds /= kf.n_split"
   ]
  }
 ]
}