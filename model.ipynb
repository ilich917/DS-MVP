{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import urllib\n",
    "import gc\n",
    "import datetime\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, path: str, train: bool = True, preprocesar = None, context = None):\n",
    "        self.preprocesar = preprocesar\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "        self.train = train\n",
    "        self.context = context\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        im_path = os.path.join(self.path, self.df.iloc[index]['image_name'] + '.jpg')\n",
    "        x = cv2.imread(im_path)\n",
    "        meta = np.array(self.df.iloc[index][self.context].values, dtype=np.float32)\n",
    "\n",
    "        if self.preprocesar:\n",
    "            x = self.preprocesar(x)\n",
    "            \n",
    "        if self.train:\n",
    "            y = self.df.iloc[index]['target']\n",
    "            return (x, meta), y\n",
    "        else:\n",
    "            return (x, meta)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body, face, handl, handr, label = data #inception input dimensions of 299x299 \n",
    "body_kp, face_kp, handl_kp, handr_kp = keypoints #csv #each cell is a vector (4 features) of keypoints all/face/handl/handr normalized\n"
   ]
  },
  {
   "source": [
    "## I've not included yet the pose/kp generation in this network, but I can."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, pretrained, N_vocabulary: int):\n",
    "        super(Red_Neuronal, self).__init__()\n",
    "\n",
    "        self.body = pretrained\n",
    "        self.face = pretrained\n",
    "        self.handl = pretrained\n",
    "        self.handr = pretrained\n",
    "\n",
    "\n",
    "        #if 'ResNet' in str(pretrained.__class__):\n",
    "        #self.pretrained.fc = nn.Linear(in_features=512, out_features=1, bias=True)\n",
    "\n",
    "        if 'Inception3' in str(pretrained.__class__):\n",
    "            self.body.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.body.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "\n",
    "            self.face.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.face.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "       \n",
    "            self.handl.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.handl.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "       \n",
    "            self.handr.AuxLogits.fc = nn.Linear(in_features = 768, out_features = 1)\n",
    "            self.handr.fc = nn.Linear(in_features = 2048, out_features = 512, bias = True)\n",
    "        \n",
    "        self.body_plus_kp = nn.Sequential(nn.Linear(512 + n_body_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "            \n",
    "        self.face_plus_kp = nn.Sequential(nn.Linear(512 + n_face_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "        \n",
    "        self.handl_plus_kp = nn.Sequential(nn.Linear(512 + n_handl_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "        \n",
    "        self.handr_plus_kp = nn.Sequential(nn.Linear(512 + n_handr_kp, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 256),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "        \n",
    "        self.encoder = nn.Sequential(nn.Linear(1024, 512),\n",
    "                                  nn.BatchNorm1d(512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.25),\n",
    "                                  nn.Linear(512, 128),\n",
    "                                  nn.BatchNorm1d(128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(p=0.2))\n",
    "\n",
    "        self.output = nn.Sequential(nn.Transformer(n,128),\n",
    "                                  nn.Linear(128, N_vocabulary))\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        The inputs are a 20 frames video plus kp for each frame. \n",
    "        This is good, because when we will test this in longer or real time video,\n",
    "        then we just need to feed it with chunks of 20 frames to make predictions.\n",
    "        When frames in video are <20, then padding (when video is starting, for example) # NOT IMPLEMENTED YET\n",
    "        \"\"\"\n",
    "              \n",
    "        n = 20 # num of frames by video / to be defined\n",
    "\n",
    "        #tensor to save the output of encoder for each frame\n",
    "        distilled_video = torch.zeros(n, 128) # (n_frames, output_encoder)\n",
    "\n",
    "        #videos of same length:\n",
    "        body, face, handl, handr = inputs[0]\n",
    "\n",
    "        # Following is the list of kp for each frame for each video [[kp_frame_1],[kp_frame_2],...,[kp_frame_n]]\n",
    "        # where kp_frame_i is the list of kp coordinates normalized\n",
    "        body_kp, face_kp, handl_kp, handr_kp = inputs[1]\n",
    "        i = 0\n",
    "        while i < n-1:\n",
    "            #CNN forward\n",
    "            body_cnn = self.body(body)\n",
    "            face_cnn = self.face(face)\n",
    "            handl_cnn = self.handl(handl)\n",
    "            handr_cnn = self.handr(handr)\n",
    "\n",
    "            #forward for CNN_output + kp\n",
    "            body_pose = self.body_plus_kp(torch.cat((body_cnn, body_kp), dim=1))\n",
    "            face_pose = self.face_plus_kp(torch.cat((face_cnn, face_kp), dim=1))\n",
    "            handl_pose = self.handl_plus_kp(torch.cat((handl_cnn, handl_kp), dim=1))\n",
    "            handr_pose = self.handr_plus_kp(torch.cat((handr_cnn, handr_kp), dim=1))\n",
    "\n",
    "            #forward for encoding the concatenation of latest forwards\n",
    "            distilled_video[i] = self.encoder(torch.cat(body_pose, face_pose, handl_pose, handr_pose))\n",
    "            i += 1\n",
    "        \n",
    "        output = F.softmax(self.output(distilled_video))\n",
    "        return output"
   ]
  }
 ]
}